# -*- coding: utf-8 -*-
"""Pytorch02 Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xrfpjpjgq2Lgr0pPw_N81JYEBRVEQFW2
"""

import torch
from torch import nn
import matplotlib.pyplot as plt

"""# Prepare the dataset for the classification data and get it ready."""

import sklearn
from sklearn.datasets import make_circles

n_samples = 1000

X, y = make_circles(n_samples,
                    noise = 0.03,
                    random_state = 42)

X[: 5]

y[: 5]

# make Dataframe of circles

import pandas as pd

circles = pd.DataFrame({'x1': X[:, 0],
                        'x2': X[:, 1],
                        'label': y})

circles.head()

plt.scatter(x = X[:, 0], y = X[:, 1], c = y, cmap = plt.cm.RdYlBu)

# Split the data into train and test
# Aim: train the netural network model to seperate the bule dot and red dot

"""# 1.1 Check input and output shapes"""

X.shape, y.shape

"""# 1.2 Turn data into tensor"""

torch.__version__

X = torch.from_numpy(X).type(torch.float)

type(X)

y = torch.from_numpy(y).type(torch.float)

X[:5], y[:5]

# Split data into train and test

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

X_train.shape, y_train.shape

"""# 2. Build a model"""

import torch
from torch import nn

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

class CircleModel0(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features = 2, out_features = 5)
    self.layer_2 = nn.Linear(in_features = 5, out_features = 1)
  def forward(self, x):
    return self.layer_2(self.layer_1(x))



model_0 = CircleModel0().to(device)
model_0

next(model_0.parameters()).device

# Replicate the model above using nn.sequential

model_0 = nn.Sequential(
    nn.Linear(in_features = 2, out_features = 5),
    nn.Linear(in_features = 5, out_features = 1)
).to(device)

model_0

model_0.state_dict()

# Make predictions
with torch.inference_mode():
  untrained_preds = model_0(X_test.to(device))
print(untrained_preds[:10])
print(untrained_preds.shape)

print(torch.round(untrained_preds[:5]))

# set up the loss function and optimizer
# for regression, you could use MAE or MSE
# but for class, you could use binary cross entropy

# for optimizer, the most usually use is tha SGD and Adam, and for loss function we can use torch.nn.BCEWithLogitsLoss

loss_fn = nn.BCEWithLogitsLoss()

optimizer = torch.optim.SGD(params = model_0.parameters(), lr = 0.1)

# Caculate accuarcy
def acc_fn(y_test, y_pred):
  correct = torch.eq(y_test, y_pred).sum().item()
  acc = (correct / len(y_pred)) * 100
  return acc

# Building model

torch.manual_seed(42)
torch.cuda.manual_seed(42)

epochs = 100

X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

for epoch in range(epochs):
  model_0.train()
  # 1. Forward pass
  y_logits = model_0(X_train).squeeze()
  y_preds = torch.round(torch.sigmoid(y_logits))

  # Caculate the loss
  loss = loss_fn(y_logits, y_train)
  acc = acc_fn(y_test = y_train, y_pred = y_preds)

  optimizer.zero_grad()

  loss.backward()

  optimizer.step()

  model_0.eval()
  with torch.inference_mode():
    test_logits = model_0(X_test).squeeze()
    test_preds = torch.round(torch.sigmoid(test_logits))

    test_loss = loss_fn(test_logits, y_test)

    test_acc = acc_fn(y_test = y_test, y_pred = test_preds)

  # Print results
  if epoch % 10 == 0:
    print(f'Epoch: {epoch} | Loss: {loss} | Acc: {acc} | Test Loss: {test_loss} | Test Acc: {test_acc}')

# Evaluate my model

import requests
from pathlib import Path
import matplotlib.pyplot as plt
# Download helper function
if Path('helper_functions.py').is_file():
  print('File already exists')
else:
  print('Downloading file')
  request = requests.get('https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py')
  with open('helper_functions.py', 'wb') as f:
    f.write(request.content)

from helper_functions import plot_predictions, plot_decision_boundary

# plot decision boundary of the model

plt.figure(figsize = (12, 6))
plt.subplot(1, 2, 1)
plt.title('Train')
plot_decision_boundary(model_0, X_train, y_train)

plt.subplot(1, 2, 2)
plt.title('Test')
plot_decision_boundary(model_0, X_test, y_test)

# Improving a model

"""* Add more layers
* Add more hidden units
* Fit for longer
* Changing the activation function
* Change the learning rate
* Change the loss function
"""

class CircleModel1(nn.Module):
  def __init__(self):
    super().__init__()

    self.layer_1 = nn.Linear(in_features = 2, out_features = 10)
    self.layer_2 = nn.Linear(in_features = 10, out_features = 10)
    self.layer_3 = nn.Linear(in_features = 10, out_features = 1)

  def forward(self, x):
    #z = self.layer_1(x)
    #z = self.layer_2(z)
    #z = self.layer_3(z)
    return self.layer_3(self.layer_2(self.layer_1(x)))

model_1 = CircleModel1().to(device)
  model_1

# Create loss function
loss_fn = nn.BCEWithLogitsLoss()
# Create optimizer
optimizer = torch.optim.SGD(params = model_1.parameters(), lr = 0.01)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

Epochs = 1000

X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

for epoch in range(Epochs):
  model_1.train()

  #1.forward fucntion
  y_logits = model_1(X_train).squeeze()
  y_preds = torch.round(torch.sigmoid(y_logits))

  #2.Caculate the loss
  loss = loss_fn(y_logits, y_train)
  acc = acc_fn(y_test = y_train, y_pred = y_preds)

  #3.Optimizer
  optimizer.zero_grad()

  #4.loss
  loss.backward()

  optimizer.step()

  model_1.eval()
  with torch.inference_mode():
    test_logits = model_1(X_test).squeeze()
    test_preds = torch.round(torch.sigmoid(test_logits))

    test_loss = loss_fn(test_logits, y_test)
    test_acc = acc_fn(y_test = y_test, y_pred = test_preds)

  model_1.eval()
  with torch.inference_mode():
    test_logits = model_1(X_test).squeeze()


  # Print out results
  if epoch % 100 == 0:
    print(f'Epoch: {epoch} | Loss: {loss} | Acc: {acc} | Test Loss: {test_loss} | Test Acc: {test_acc}')

plt.figure(figsize = (12, 6))
plt.subplot(1, 2, 1)
plt.title('Train')
plot_decision_boundary(model_1, X_train, y_train)

plt.subplot(1, 2, 2)
plt.title('Test')
plot_decision_boundary(model_1, X_test, y_test)

"""## Missing: Non-linearity"""

from sklearn.datasets import make_circles

n_samples = 1000
X, y = make_circles(n_samples,
                    noise = 0.03,
                    random_state = 42)
plt.scatter(X[:, 0], X[:, 1], c = y, cmap = plt.cm.RdYlBu)

import torch
from sklearn.model_selection import train_test_split
from torch import nn

X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)
X_train[:5], y_train[:5]

"""## Non-linear model"""

import torch
from torch import nn

class CircleModel2(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features = 2, out_features = 10)
    self.layer_2 = nn.Linear(in_features = 10, out_features = 10)
    self.layer_3 = nn.Linear(in_features = 10, out_features = 1)
    self.relu = nn.ReLU()

  def forward(self, x):
    return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))

model_3 = CircleModel2().to(device)
model_3

from torch import nn
class CircleModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features=2, out_features=10)
    self.layer_2 = nn.Linear(in_features=10, out_features=10)
    self.layer_3 = nn.Linear(in_features=10, out_features=1)
    self.relu = nn.ReLU() # relu is a non-linear activation function

  def forward(self, x):
    # Where should we put our non-linear activation functions?
    return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))

model_3 = CircleModel2().to(device)
model_3

loss_fn = nn.BCEWithLogitsLoss()
optiimizer = torch.optim.SGD(params = model_3.parameters(), lr = 00.1)

len(X_train)

def acc_fn(y_true, y_pred):
    correct = (y_true == y_pred).sum().item()
    return correct / len(y_true)

torch.manual_seed(42)
torch.cuda.manual_seed(42)

X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

epochs = 1000

for epoch in range(epochs):
  model_3.train()

  # forward
  y_logits = model_3(X_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))

  #caculate the loss
  loss = loss_fn(y_logits, y_train)
  acc = acc_fn(y_true = y_train, y_pred = y_pred)

  #zero
  optimizer.zero_grad()

  #loss backword
  loss.backward()

  #set
  optimizer.step()

  ## Testing
  model_3.eval()
  with torch.inference_mode():
    test_logits = model_3(X_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

    test_loss = loss_fn(test_logits, y_test)
    test_acc = acc_fn(y_true = y_test, y_pred = test_pred)

  if epoch % 100 ==0:
    print(f'Epoch: {epoch} | Loss: {loss} | Acc: {acc} | Test Loss: {test_loss} | Test Acc: {test_acc}')





# Make and plot data
import matplotlib.pyplot as plt
from sklearn.datasets import make_circles

n_samples = 1000

X, y = make_circles(n_samples,
                    noise=0.03,
                    random_state=42)

plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdYlBu);

# Convert data to tensors and then to train and test splits
import torch
from sklearn.model_selection import train_test_split

# Turn data into tensors
X = torch.from_numpy(X).type(torch.float)
y = torch.from_numpy(y).type(torch.float)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=0.2,
                                                    random_state=42)

X_train[:5], y_train[:5]

# Build a model with non-linear activation functions
from torch import nn
class CircleModelV2(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_1 = nn.Linear(in_features=2, out_features=10)
    self.layer_2 = nn.Linear(in_features=10, out_features=10)
    self.layer_3 = nn.Linear(in_features=10, out_features=1)
    self.relu = nn.ReLU() # relu is a non-linear activation function

  def forward(self, x):
    # Where should we put our non-linear activation functions?
    return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))

model_3 = CircleModelV2().to(device)
model_3

# Setup loss and optimizer
loss_fn = nn.BCEWithLogitsLoss()
optimizer = torch.optim.SGD(model_3.parameters(),
                            lr=0.1)

def accuracy_fn(y_true, y_pred):
  correct = torch.eq(y_true, y_pred).sum().item()
  acc = (correct/len(y_pred)) * 100
  return acc

# Random seeds
torch.manual_seed(42)
torch.cuda.manual_seed(42)

# Put all data on target device
X_train, y_train = X_train.to(device), y_train.to(device)
X_test, y_test = X_test.to(device), y_test.to(device)

# Loop through data
epochs = 1000

for epoch in range(epochs):
  ### Training
  model_3.train()

  # 1. Forward pass
  y_logits = model_3(X_train).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits)) # logits -> prediction probabilities -> prediction labels

  # 2. Calculate the loss
  loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss (takes in logits as first input)
  acc = accuracy_fn(y_true=y_train,
                    y_pred=y_pred)

  # 3. Optimizer zero grad
  optimizer.zero_grad()

  # 4. Loss backward
  loss.backward()

  # 5. Step the optimizer
  optimizer.step()

  ### Testing
  model_3.eval()
  with torch.inference_mode():
    test_logits = model_3(X_test).squeeze()
    test_pred = torch.round(torch.sigmoid(test_logits))

    test_loss = loss_fn(test_logits, y_test)
    test_acc = accuracy_fn(y_true=y_test,
                           y_pred=test_pred)

  # Print out what's this happenin'
  if epoch % 100 == 0:
    print(f"Epoch: {epoch} | Loss: {loss:.4f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%")

# Evaluating our model (visualizing)

model_3.eval()
with torch.inference_mode():
  y_logits = model_3(X_test).squeeze()
  y_pred = torch.round(torch.sigmoid(y_logits))
y_test[:5], y_pred[:5]

plt.figure(figsize = (12, 6))
plt.subplot(1, 2, 1)
plt.title('Train')
plot_decision_boundary(model_3, X_train, y_train)

plt.subplot(1, 2, 2)
plt.title('Test')
plot_decision_boundary(model_3, X_test, y_test)

"""## Replicate the non-linear model"""

# Create a tensor
A = torch.arange(-10, 10, 1.0)
A.dtype

A

plt.plot(A)

plt.plot(torch.relu(A))

plt.plot(torch.sigmoid(A))

# Put it all together with multi-class classification

"""#8.1 Create a toy multi-class classification"""

from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split

num_classes = 4
num_features = 2
random_seed = 42

X_blob, y_blob = make_blobs(n_samples = 1000,
                           n_features = num_features,
                           centers = num_classes,
                           cluster_std = 1.5,
                           random_state = random_seed)

# turn data into tensors
x_blob = torch.from_numpy(X_blob).type(torch.float)
y_blob = torch.from_numpy(y_blob).type(torch.float)

X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,
                                                                        y_blob,
                                                                        test_size = 0.2,
                                                                        random_state = random_seed)

X_blob[0]

import matplotlib.pyplot as plt
plt.figure(figsize = (10, 7))
plt.scatter(X_blob[:, 0], X_blob[:, 1], c = y_blob, cmap = plt.cm.RdYlBu)

"""## Building multi-class classification model in pytorch"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'
device

class BlobModel(nn.Module):
  def __init__(self, input_features, output_features, hidden_units = 0):
    super().__init__()
    self.linear_layer_stack = nn.Sequential(
        nn.Linear(in_features = input_features, out_features = hidden_units),
        nn.ReLU(),
        nn.Linear(in_features = hidden_units, out_features = hidden_units),
        nn.ReLU(),
        nn.Linear(in_features = hidden_units, out_features = output_features)
        )
    def forward(self, x):
      return self.linear_layer_stack(x)

model_4 = BlobModel(input_features = 2, output_features = 4, hidden_units = 8).to(device)

